{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5765407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import png\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Upsample\n",
    "from torch.nn import Conv2d as Conv2D\n",
    "from PIL import Image\n",
    "from colormap.colors import Color, hex2rgb\n",
    "from sklearn.metrics import average_precision_score as ap_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5462d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions.\n",
    "\"\"\"\n",
    "def save_label(label, path):\n",
    "    '''\n",
    "    Function for ploting labels.\n",
    "    '''\n",
    "    colormap = [\n",
    "        '#000000',\n",
    "        '#0080FF',\n",
    "        '#80FF80',\n",
    "        '#FF8000',\n",
    "        '#FF0000',\n",
    "    ]\n",
    "    assert(np.max(label)<len(colormap))\n",
    "    colors = [hex2rgb(color, normalise=False) for color in colormap]\n",
    "    w = png.Writer(label.shape[1], label.shape[0], palette=colors, bitdepth=4)\n",
    "    with open(path, 'wb') as f:\n",
    "        w.write(f, label)\n",
    "\n",
    "def train(trainloader, net, criterion, optimizer, device, epoch):\n",
    "    '''\n",
    "    Function for training.\n",
    "    '''\n",
    "    start = time.time()\n",
    "    running_loss = 0.0\n",
    "    net = net.train()\n",
    "    for images, labels in tqdm(trainloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss = loss.item()\n",
    "    end = time.time()\n",
    "    print('[epoch %d] loss: %.3f elapsed time %.3f' %\n",
    "          (epoch, running_loss, end-start))\n",
    "    return running_loss\n",
    "\n",
    "def test(testloader, net, criterion, device):\n",
    "    '''\n",
    "    Function for testing.\n",
    "    '''\n",
    "    losses = 0.\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        net = net.eval()\n",
    "        for images, labels in tqdm(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = net(images)\n",
    "            loss = criterion(output, labels)\n",
    "            losses += loss.item()\n",
    "            cnt += 1\n",
    "    print(losses / cnt)\n",
    "    return (losses/cnt)\n",
    "\n",
    "def cal_AP(testloader, net, criterion, device):\n",
    "    '''\n",
    "    Calculate Average Precision\n",
    "    '''\n",
    "    losses = 0.\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        net = net.eval()\n",
    "        preds = [[] for _ in range(5)]\n",
    "        heatmaps = [[] for _ in range(5)]\n",
    "        for images, labels in tqdm(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = net(images).cpu().numpy()\n",
    "            for c in range(5):\n",
    "                preds[c].append(output[:, c].reshape(-1))\n",
    "                heatmaps[c].append(labels[:, c].cpu().numpy().reshape(-1))\n",
    "\n",
    "        aps = []\n",
    "        for c in range(5):\n",
    "            preds[c] = np.concatenate(preds[c])\n",
    "            heatmaps[c] = np.concatenate(heatmaps[c])\n",
    "            if heatmaps[c].max() == 0:\n",
    "                ap = float('nan')\n",
    "            else:\n",
    "                ap = ap_score(heatmaps[c], preds[c])\n",
    "                aps.append(ap)\n",
    "            print(\"AP = {}\".format(ap))\n",
    "\n",
    "    # print(losses / cnt)\n",
    "    return None\n",
    "\n",
    "def get_result(testloader, net, device, folder='./part3/output_train'):\n",
    "    result = []\n",
    "    cnt = 1\n",
    "    with torch.no_grad():\n",
    "        net = net.eval()\n",
    "        cnt = 0\n",
    "        for images, labels in tqdm(testloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = net(images)[0].cpu().numpy()\n",
    "            c, h, w = output.shape\n",
    "            assert(c == N_CLASS)\n",
    "            y = np.zeros((h,w)).astype('uint8')\n",
    "            for i in range(N_CLASS):\n",
    "                mask = output[i]>0.5\n",
    "                y[mask] = i\n",
    "            gt = labels.cpu().data.numpy().squeeze(0).astype('uint8')\n",
    "            save_label(y, './{}/y{}.png'.format(folder, cnt))\n",
    "            save_label(gt, './{}/gt{}.png'.format(folder, cnt))\n",
    "            plt.imsave(\n",
    "                './{}/x{}.png'.format(folder, cnt),\n",
    "                ((images[0].cpu().data.numpy()+1)*128).astype(np.uint8).transpose(1,2,0))\n",
    "\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9729b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset.\n",
    "\"\"\"\n",
    "class FacadeDataset(Dataset):\n",
    "    def __init__(self, flag, dataDir='./part3/starter_set/', data_range=(0, 8), n_class=5, onehot=False):\n",
    "        self.onehot = onehot\n",
    "        assert(flag in ['train', 'eval', 'test', 'test_dev', 'kaggle'])\n",
    "        print(\"load \"+ flag+\" dataset start\")\n",
    "        print(\"    from: %s\" % dataDir)\n",
    "        print(\"    range: [%d, %d)\" % (data_range[0], data_range[1]))\n",
    "        self.dataset = []\n",
    "        for i in range(data_range[0], data_range[1]):\n",
    "            img = Image.open(os.path.join(dataDir,flag,'eecs442_%04d.jpg' % i))\n",
    "\n",
    "            pngreader = png.Reader(filename=os.path.join(dataDir,flag,'eecs442_%04d.png' % i))\n",
    "            w,h,row,info = pngreader.read()\n",
    "            label = np.array(list(row)).astype('uint8')\n",
    "\n",
    "            # Normalize input image\n",
    "            img = np.asarray(img).astype(\"f\").transpose(2, 0, 1)/128.0-1.0\n",
    "            # Convert to n_class-dimensional onehot matrix\n",
    "            label_ = np.asarray(label)\n",
    "            label = np.zeros((n_class, img.shape[1], img.shape[2])).astype(\"i\")\n",
    "            for j in range(n_class):\n",
    "                label[j, :] = label_ == j\n",
    "            self.dataset.append((img, label))\n",
    "        print(\"load dataset done\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.dataset[index]\n",
    "        label = torch.FloatTensor(label)\n",
    "        if not self.onehot:\n",
    "            label = torch.argmax(label, dim=0)\n",
    "        else:\n",
    "            label = label.long()\n",
    "\n",
    "        return torch.FloatTensor(img), torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d62b7bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train dataset start\n",
      "    from: ./part3/starter_set/\n",
      "    range: [0, 905)\n",
      "load dataset done\n",
      "load test_dev dataset start\n",
      "    from: ./part3/starter_set/\n",
      "    range: [0, 57)\n",
      "load dataset done\n",
      "load test_dev dataset start\n",
      "    from: ./part3/starter_set/\n",
      "    range: [57, 114)\n",
      "load dataset done\n",
      "load test_dev dataset start\n",
      "    from: ./part3/starter_set/\n",
      "    range: [57, 114)\n",
      "load dataset done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DataLoaders.\n",
    "\"\"\"\n",
    "# batch_size.\n",
    "batch_size = 32\n",
    "\n",
    "# training dataloader\n",
    "train_data = FacadeDataset(\n",
    "    flag='train',\n",
    "    data_range=(0, 905),\n",
    "    onehot=False,\n",
    ")\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "# validation dataloader\n",
    "val_data = FacadeDataset(\n",
    "    flag='test_dev',\n",
    "    data_range=(0, 57),\n",
    "    onehot=False\n",
    ")\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "# test dataloader\n",
    "test_data = FacadeDataset(\n",
    "    flag='test_dev',\n",
    "    data_range=(57, 114),\n",
    "    onehot=False\n",
    ")\n",
    "test_loader = DataLoader(test_data, batch_size=1)\n",
    "\n",
    "# AP dataloader\n",
    "ap_data = FacadeDataset(\n",
    "    flag='test_dev',\n",
    "    data_range=(57, 114),\n",
    "    onehot=True\n",
    ")\n",
    "ap_loader = DataLoader(ap_data, batch_size=1)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210b3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN model.\n",
    "\"\"\"\n",
    "N_CLASS=5\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_class = N_CLASS\n",
    "        kernel_size = 1\n",
    "        padding = 1\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layers = nn.Sequential(\n",
    "            # encoder\n",
    "            nn.Conv2d(3, 64, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            nn.Conv2d(64, 64, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            self.pool,\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            nn.Conv2d(128, 128, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            self.pool,\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            nn.Conv2d(128, 256, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            self.pool,\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            nn.Conv2d(512, 512, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            self.pool,\n",
    "            \n",
    "            nn.Conv2d(512, 1024, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            nn.Conv2d(1024, 1024, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            self.pool,\n",
    "            \n",
    "            # decoder\n",
    "            # nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
    "            nn.Conv2d(1024, 512, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            nn.Conv2d(512, 512, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            \n",
    "            # nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 256, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            \n",
    "            # nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 128, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            nn.Conv2d(128, 128, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            \n",
    "            # nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 64, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            nn.Conv2d(64, 64, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            \n",
    "            # nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 3, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            nn.Conv2d(3, 3, kernel_size=kernel_size, padding=padding),\n",
    "            self.relu,\n",
    "            \n",
    "            # output\n",
    "            nn.Conv2d(3, self.n_class, kernel_size=1, padding=0),\n",
    "            self.relu,\n",
    "        )\n",
    "    \n",
    "    def upsample(self, input_size, output_size):\n",
    "        x1 = nn.ConvTranspose2d(input_size, output_size, kernel_size=2, stride=2) \n",
    "        y = torch.cat([])\n",
    "        return y\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4ff98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=3, padding=1):\n",
    "        super(Up, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2D(\n",
    "                channel_in,\n",
    "                channel_out,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "            ),\n",
    "            nn.BatchNorm2d(channel_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # upsample using bilinear mode and scale it to twice its size\n",
    "        x1 = self.upsample(x1)\n",
    "        difference_in_X = x1.size()[2] - x2.size()[2]\n",
    "        difference_in_Y = x1.size()[3] - x2.size()[3]\n",
    "        # padding with the required value\n",
    "        x2 = F.pad(x2, (\n",
    "            difference_in_X // 2, int(difference_in_X / 2),\n",
    "            difference_in_Y // 2, int(difference_in_Y / 2)\n",
    "        ))\n",
    "        # concat on channel axis\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        # convolve\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, kernel_size=3, padding=1):\n",
    "        super(Down, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2D(\n",
    "                channel_in,\n",
    "                channel_out,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "            ),\n",
    "            nn.BatchNorm2d(channel_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # downsample\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # convolve\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, channel_in, classes, kernel_size, padding):\n",
    "        super(UNet, self).__init__()\n",
    "        self.input_conv = self.conv = nn.Sequential(\n",
    "            Conv2D(\n",
    "                channel_in,\n",
    "                64,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 256)\n",
    "        self.up1 = Up(512, 128)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.up3 = Up(128, 32)\n",
    "        self.output_conv = nn.Conv2d(\n",
    "            32,\n",
    "            classes,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.input_conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        output = self.output_conv(x)\n",
    "        return F.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6202942a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training\n",
      "-----------------Epoch = 1-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:46<00:00, 18.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 1.385 elapsed time 526.608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4614538550376892\n",
      "-----------------Epoch = 2-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:44<00:00, 18.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2] loss: 1.299 elapsed time 524.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3524540662765503\n",
      "-----------------Epoch = 3-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:45<00:00, 18.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3] loss: 1.236 elapsed time 525.271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2887605428695679\n",
      "-----------------Epoch = 4-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:44<00:00, 18.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4] loss: 1.188 elapsed time 524.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2396795749664307\n",
      "-----------------Epoch = 5-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:45<00:00, 18.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 5] loss: 1.160 elapsed time 525.436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2235507369041443\n",
      "-----------------Epoch = 6-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:43<00:00, 18.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 6] loss: 1.129 elapsed time 523.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.200247585773468\n",
      "-----------------Epoch = 7-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:45<00:00, 18.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 7] loss: 1.110 elapsed time 525.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1831563711166382\n",
      "-----------------Epoch = 8-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:48<00:00, 18.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 8] loss: 1.102 elapsed time 528.309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1817044615745544\n",
      "-----------------Epoch = 9-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:48<00:00, 18.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 9] loss: 1.083 elapsed time 528.268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1862441897392273\n",
      "-----------------Epoch = 10-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:45<00:00, 18.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 10] loss: 1.072 elapsed time 525.193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1954768300056458\n",
      "-----------------Epoch = 11-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:48<00:00, 18.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 11] loss: 1.062 elapsed time 528.254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1559399366378784\n",
      "-----------------Epoch = 12-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:46<00:00, 18.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 12] loss: 1.056 elapsed time 526.254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1757326126098633\n",
      "-----------------Epoch = 13-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:47<00:00, 18.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 13] loss: 1.050 elapsed time 527.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.168344795703888\n",
      "-----------------Epoch = 14-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:46<00:00, 18.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 14] loss: 1.042 elapsed time 526.270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1688479781150818\n",
      "-----------------Epoch = 15-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [08:48<00:00, 18.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 15] loss: 1.037 elapsed time 528.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2/2 [00:11<00:00,  5.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1711574792861938\n",
      "\n",
      "Finished Training, Testing on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 57/57 [00:11<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1620285511016846\n",
      "\n",
      "Generating Unlabeled Result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 57/57 [00:19<00:00,  2.99it/s]\n",
      "100%|███████████████████████████████████████████████████████| 57/57 [00:11<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP = 0.648458035344062\n",
      "AP = 0.7666110294884456\n",
      "AP = 0.0644204207887888\n",
      "AP = 0.8554949157525819\n",
      "AP = 0.6396604625185022\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main function.\n",
    "\"\"\"\n",
    "# init model\n",
    "name = 'starter_net'\n",
    "net = UNet(\n",
    "    channel_in=3,\n",
    "    classes=5,\n",
    "    kernel_size=3,\n",
    "    padding=1,\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss() #TODO decide loss\n",
    "optimizer = torch.optim.Adam(\n",
    "    net.parameters(),\n",
    "    1e-3,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "n_epochs = 15\n",
    "results = []\n",
    "\n",
    "# train model\n",
    "print('\\nStart training')\n",
    "for epoch in range(n_epochs):\n",
    "    print('-----------------Epoch = %d-----------------' % (epoch+1))\n",
    "    # train\n",
    "    train_loss = train(\n",
    "        train_loader,\n",
    "        net,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device,\n",
    "        epoch+1\n",
    "    )\n",
    "    # validate\n",
    "    val_loss = test(val_loader, net, criterion, device)\n",
    "    # append results\n",
    "    results.append({\n",
    "        'i': epoch,\n",
    "        'training_loss': train_loss,\n",
    "        'validation_loss': val_loss,\n",
    "    })\n",
    "    \n",
    "\n",
    "# test trained model\n",
    "print('\\nFinished Training, Testing on test set')\n",
    "test(test_loader, net, criterion, device)\n",
    "\n",
    "print('\\nGenerating Unlabeled Result')\n",
    "result = get_result(test_loader, net, device, folder='./part3/output_test')\n",
    "\n",
    "# save trained model\n",
    "torch.save(\n",
    "    net.state_dict(),\n",
    "    './part3/models/model_{}.pth'.format(name)\n",
    ")\n",
    "\n",
    "# calculate average precision\n",
    "cal_AP(ap_loader, net, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614541b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
