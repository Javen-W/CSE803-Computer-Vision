{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502a37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Displays a progress bar\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29fa789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 9912422/9912422 [00:02<00:00, 4386004.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\train-images-idx3-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 28881/28881 [00:00<00:00, 925045.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\train-labels-idx1-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 6664769.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and train, val, test splits\n",
    "print(\"Loading datasets...\")\n",
    "dataset_path = \"C:/Users/Admin/Desktop/cse803_hw5\"\n",
    "\n",
    "MNIST_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.1307], [0.3081])\n",
    "])\n",
    "MNIST_train = datasets.MNIST(\n",
    "    dataset_path,\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=MNIST_transform\n",
    ")\n",
    "MNIST_test = datasets.MNIST(\n",
    "    dataset_path,\n",
    "    download=True,\n",
    "    train = False,\n",
    "    transform=MNIST_transform\n",
    ")\n",
    "\n",
    "FASHION_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.2859], [0.3530])\n",
    "])\n",
    "FASHION_train = datasets.FashionMNIST(\n",
    "    dataset_path,\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=MNIST_transform\n",
    ")\n",
    "FASHION_test = datasets.FashionMNIST(\n",
    "    dataset_path,\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=FASHION_transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12972f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Loaders.\n",
    "\"\"\"\n",
    "class GridDataset(Dataset):\n",
    "    def __init__(self, MNIST_dataset, FASHION_dataset): # pass in dataset\n",
    "        assert len(MNIST_dataset) == len(FASHION_dataset)\n",
    "        self.MNIST_dataset, self.FASHION_dataset = MNIST_dataset, FASHION_dataset\n",
    "        self.targets = FASHION_dataset.targets\n",
    "        torch.manual_seed(442) # Fix random seed for reproducibility\n",
    "        N = len(MNIST_dataset)\n",
    "        self.randpos = torch.randint(low=0,high=4,size=(N,)) # position of the FASHION-MNIST image\n",
    "        self.randidx = torch.randint(low=0,high=N,size=(N,3)) # indices of MNIST images\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.MNIST_dataset)\n",
    "    \n",
    "    def __getitem__(self,idx): # Get one Fashion-MNIST image and three MNIST images to make a new image\n",
    "        idx1, idx2, idx3 = self.randidx[idx]\n",
    "        x = self.randpos[idx]%2\n",
    "        y = self.randpos[idx]//2\n",
    "        p1 = self.FASHION_dataset.__getitem__(idx)[0]\n",
    "        p2 = self.MNIST_dataset.__getitem__(idx1)[0]\n",
    "        p3 = self.MNIST_dataset.__getitem__(idx2)[0]\n",
    "        p4 = self.MNIST_dataset.__getitem__(idx3)[0]\n",
    "        combo = torch.cat((torch.cat((p1,p2),2),torch.cat((p3,p4),2)),1)\n",
    "        combo = torch.roll(combo, (x*28,y*28), dims=(0,1))\n",
    "        return (combo,self.targets[idx])\n",
    "\n",
    "trainset = GridDataset(MNIST_train, FASHION_train)\n",
    "testset = GridDataset(MNIST_test, FASHION_test)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6aa5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Network class.\n",
    "\"\"\"\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Design your own base module, define layers here\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(1,32,5,1,2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        out_channel = 32 # TODO: Put the output channel number of your base module here\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(out_channel,10)\n",
    "        self.conv = nn.Conv2d(out_channel,10,1) # 1x1 conv layer (substitutes fc)\n",
    "\n",
    "    def transfer(self): # Copy weights of fc layer into 1x1 conv layer\n",
    "        self.conv.weight = nn.Parameter(self.fc.weight.unsqueeze(2).unsqueeze(3))\n",
    "        self.conv.bias = nn.Parameter(self.fc.bias)\n",
    "\n",
    "    def visualize(self,x):\n",
    "        x = self.base(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.base(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b535916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameters.\n",
    "\"\"\"\n",
    "# configure device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# init model\n",
    "model = Network().to(device)\n",
    "\n",
    "# specify the loss layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: Modify the line below, experiment with different optimizers and parameters (such as learning rate)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# TODO: choose an appropriate number of training epochs\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94087230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train & evaluation functions.\n",
    "\"\"\"\n",
    "def train(model, train_loader, val_loader, num_epoch = 10): # Train the model\n",
    "    print(\"Start training...\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for i in range(num_epoch):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        for batch, label in tqdm(train_loader):\n",
    "            # format data\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # Clear gradients from the previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # This will call Network.forward() that you implement\n",
    "            pred = model(batch)\n",
    "            \n",
    "            # Calculate the training loss\n",
    "            loss = criterion(pred, label)\n",
    "            running_loss.append(loss.item())\n",
    "            \n",
    "            # Backprop gradients to all tensors in the network\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update trainable weights\n",
    "            optimizer.step()\n",
    "        \n",
    "        # training loss\n",
    "        train_loss = np.mean(running_loss)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # validation loss\n",
    "        _, val_loss = evaluate(model, val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # report epoch results\n",
    "        print(f\"Epoch {i+1}: train_loss={train_loss}, val_loss={val_loss}\") # Print the average losses for this epoch\n",
    "    \n",
    "    # finished\n",
    "    print(\"Done!\")\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate(model, val_loader): # Evaluate accuracy on validation / test set\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    running_loss = []\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # Do not calculate grident to speed up computation\n",
    "        for batch, label in tqdm(val_loader):\n",
    "            # format data\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # make predictions\n",
    "            pred = model(batch)\n",
    "            \n",
    "            # Calculate the validation loss\n",
    "            loss = criterion(pred, label)\n",
    "            running_loss.append(loss.item())\n",
    "            \n",
    "            # calculate batch accuracy\n",
    "            correct += (torch.argmax(pred,dim=1)==label).sum().item()\n",
    "    \n",
    "    # averaged accuracy\n",
    "    acc = correct / len(val_loader.dataset)\n",
    "    \n",
    "    # validation loss\n",
    "    val_loss = np.mean(running_loss)\n",
    "    \n",
    "    # finished\n",
    "    print(\"Evaluation accuracy: {}\".format(acc))\n",
    "    return acc, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd3036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train and evaluate model.\n",
    "\"\"\"\n",
    "# train\n",
    "train_losses, val_losses = train(\n",
    "    model,\n",
    "    trainloader,\n",
    "    valloader,\n",
    "    num_epoch\n",
    ")\n",
    "\n",
    "print(\"Evaluate on test set\")\n",
    "test_acc, test_loss = evaluate(\n",
    "    model,\n",
    "    testloader\n",
    ")\n",
    "\n",
    "model.transfer() # Copy the weights from fc layer to 1x1 conv layer\n",
    "\n",
    "# TODO: Choose a correctly classified image and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analyze training & evaluation results.\n",
    "\"\"\"\n",
    "results = []\n",
    "for i, (t_loss, v_loss) in enumerate(zip(train_losses, val_losses)):\n",
    "    results.append({\n",
    "        'epoch': i,\n",
    "        'training_loss': t_loss,\n",
    "        'validation_loss': v_loss,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame.from_records(results).set_index('epoch')\n",
    "print(results_df)\n",
    "\n",
    "# plot figure\n",
    "results_df.plot(\n",
    "    xlabel=\"Epoch\",\n",
    "    ylabel=\"Loss\",\n",
    "    grid=True,\n",
    ")\n",
    "plt.title(\"Q2: Training Loss vs Epoch\", fontsize=10)\n",
    "plt.savefig(f\"./figures/{\"q2_losses\"}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7221a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
