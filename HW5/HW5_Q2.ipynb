{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502a37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Displays a progress bar\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29fa789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 9912422/9912422 [00:02<00:00, 4386004.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\train-images-idx3-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 28881/28881 [00:00<00:00, 925045.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\train-labels-idx1-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 6664769.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:/Users/Admin/Desktop/cse803_hw5\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and train, val, test splits\n",
    "print(\"Loading datasets...\")\n",
    "dataset_path = \"C:/Users/Admin/Desktop/cse803_hw5\"\n",
    "\n",
    "MNIST_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.1307], [0.3081])\n",
    "])\n",
    "MNIST_train = datasets.MNIST(\n",
    "    dataset_path,\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=MNIST_transform\n",
    ")\n",
    "MNIST_test = datasets.MNIST(\n",
    "    dataset_path,\n",
    "    download=True,\n",
    "    train = False,\n",
    "    transform=MNIST_transform\n",
    ")\n",
    "\n",
    "FASHION_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.2859], [0.3530])\n",
    "])\n",
    "FASHION_train = datasets.FashionMNIST(\n",
    "    dataset_path,\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=MNIST_transform\n",
    ")\n",
    "FASHION_test = datasets.FashionMNIST(\n",
    "    dataset_path,\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=FASHION_transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12972f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Loaders.\n",
    "\"\"\"\n",
    "class GridDataset(Dataset):\n",
    "    def __init__(self, MNIST_dataset, FASHION_dataset): # pass in dataset\n",
    "        assert len(MNIST_dataset) == len(FASHION_dataset)\n",
    "        self.MNIST_dataset, self.FASHION_dataset = MNIST_dataset, FASHION_dataset\n",
    "        self.targets = FASHION_dataset.targets\n",
    "        torch.manual_seed(442) # Fix random seed for reproducibility\n",
    "        N = len(MNIST_dataset)\n",
    "        self.randpos = torch.randint(low=0,high=4,size=(N,)) # position of the FASHION-MNIST image\n",
    "        self.randidx = torch.randint(low=0,high=N,size=(N,3)) # indices of MNIST images\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.MNIST_dataset)\n",
    "    \n",
    "    def __getitem__(self,idx): # Get one Fashion-MNIST image and three MNIST images to make a new image\n",
    "        idx1, idx2, idx3 = self.randidx[idx]\n",
    "        x = self.randpos[idx]%2\n",
    "        y = self.randpos[idx]//2\n",
    "        p1 = self.FASHION_dataset.__getitem__(idx)[0]\n",
    "        p2 = self.MNIST_dataset.__getitem__(idx1)[0]\n",
    "        p3 = self.MNIST_dataset.__getitem__(idx2)[0]\n",
    "        p4 = self.MNIST_dataset.__getitem__(idx3)[0]\n",
    "        combo = torch.cat((torch.cat((p1,p2),2),torch.cat((p3,p4),2)),1)\n",
    "        combo = torch.roll(combo, (x*28,y*28), dims=(0,1))\n",
    "        return (combo,self.targets[idx])\n",
    "\n",
    "trainset = GridDataset(MNIST_train, FASHION_train)\n",
    "testset = GridDataset(MNIST_test, FASHION_test)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6aa5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Network class.\n",
    "\"\"\"\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Design your own base module, define layers here\n",
    "        k_size = 5\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=k_size, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=k_size, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=k_size, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        out_channel = 128\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(out_channel,10)\n",
    "        self.conv = nn.Conv2d(out_channel,10,1) # 1x1 conv layer (substitutes fc)\n",
    "\n",
    "    def transfer(self): # Copy weights of fc layer into 1x1 conv layer\n",
    "        self.conv.weight = nn.Parameter(self.fc.weight.unsqueeze(2).unsqueeze(3))\n",
    "        self.conv.bias = nn.Parameter(self.fc.bias)\n",
    "\n",
    "    def visualize(self,x):\n",
    "        x = self.base(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.base(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b535916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameters.\n",
    "\"\"\"\n",
    "# configure device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# init model\n",
    "model = Network().to(device)\n",
    "\n",
    "# specify the loss layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: Modify the line below, experiment with different optimizers and parameters (such as learning rate)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# TODO: choose an appropriate number of training epochs\n",
    "num_epoch = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94087230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train & evaluation functions.\n",
    "\"\"\"\n",
    "def train(model, loader, num_epoch = 10): # Train the model\n",
    "    print(\"Start training...\")\n",
    "    model.train() # Set the model to training mode\n",
    "    for i in range(num_epoch):\n",
    "        running_loss = []\n",
    "        for batch, label in tqdm(loader):\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad() # Clear gradients from the previous iteration\n",
    "            pred = model(batch) # This will call Network.forward() that you implement\n",
    "            loss = criterion(pred, label) # Calculate the loss\n",
    "            running_loss.append(loss.item())\n",
    "            loss.backward() # Backprop gradients to all tensors in the network\n",
    "            optimizer.step() # Update trainable weights\n",
    "        print(\"Epoch {} loss:{}\".format(i+1,np.mean(running_loss))) # Print the average loss for this epoch\n",
    "    print(\"Done!\")\n",
    "\n",
    "def evaluate(model, loader): # Evaluate accuracy on validation / test set\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # Do not calculate grident to speed up computation\n",
    "        for batch, label in tqdm(loader):\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = model(batch)\n",
    "            correct += (torch.argmax(pred,dim=1)==label).sum().item()\n",
    "    acc = correct/len(loader.dataset)\n",
    "    print(\"Evaluation accuracy: {}\".format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82cd3036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 938/938 [05:06<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss:0.9900219045214053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 938/938 [05:13<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss:0.5521289967715359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 938/938 [06:18<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss:0.43913425810174395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 938/938 [08:40<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss:0.3843382164232258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 938/938 [14:30<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss:0.3522641232717774\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# train & evaluate\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train(model, trainloader, num_epoch)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Copy the weights from fc layer to 1x1 conv layer\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtransfer()\n",
      "\u001b[1;31mTypeError\u001b[0m: evaluate() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train and evaluate model.\n",
    "\"\"\"\n",
    "# train\n",
    "train(model, trainloader, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1df8020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 938/938 [14:42<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss:0.3304809541590432\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, trainloader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84cbe854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 157/157 [02:05<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluate\n",
    "\"\"\"\n",
    "evaluate(model, testloader)\n",
    "\n",
    "# Copy the weights from fc layer to 1x1 conv layer\n",
    "model.transfer()\n",
    "\n",
    "# TODO: Choose a correctly classified image and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cbb6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
